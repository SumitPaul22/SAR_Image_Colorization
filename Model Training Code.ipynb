{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.10.14","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"gpu","dataSources":[{"sourceId":2008381,"sourceType":"datasetVersion","datasetId":1201791},{"sourceId":135642,"sourceType":"modelInstanceVersion","modelInstanceId":114744,"modelId":138009}],"dockerImageVersionId":30787,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":true}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":"# Use This, if you wanna train all three Generator, Discriminator & Classifier models.","metadata":{}},{"cell_type":"code","source":"# Necessary imports\nimport os\nimport torch\nfrom torch import nn\nimport torch.optim as optim\nfrom torch.utils.data import DataLoader, Dataset\nfrom torchvision import transforms, models\nimport torch.nn.functional as F\nfrom PIL import Image\nimport matplotlib.pyplot as plt\nfrom torch.cuda.amp import GradScaler, autocast\n\n# Kaggle paths for dataset and checkpoint\nbase_path = r\"/kaggle/input/sentinel12-image-pairs-segregated-by-terrain/v_2\"\ncheckpoint_dir = r\"/kaggle/working/SAR_Model\"\nif not os.path.exists(checkpoint_dir):\n    os.makedirs(checkpoint_dir)\n\n# Dataset class to handle both SAR (grayscale) and optical (RGB) image pairs\nclass SARImageDataset(Dataset):\n    def __init__(self, base_path, transform=None):\n        self.s1_paths = []\n        self.s2_paths = []\n        self.labels = []\n\n        categories = ['agri', 'barrenland', 'grassland', 'urban']\n        category_to_label = {cat: i for i, cat in enumerate(categories)}\n        \n        for category in categories:\n            s1_folder = os.path.join(base_path, category, 's1')\n            s2_folder = os.path.join(base_path, category, 's2')\n            self.s1_paths += sorted([os.path.join(s1_folder, img) for img in os.listdir(s1_folder)])\n            self.s2_paths += sorted([os.path.join(s2_folder, img) for img in os.listdir(s2_folder)])\n            self.labels += [category_to_label[category]] * len(os.listdir(s1_folder))\n        \n        self.transform = transform\n\n    def __len__(self):\n        return len(self.s1_paths)\n\n    def __getitem__(self, idx):\n        s1_image = Image.open(self.s1_paths[idx]).convert('L')  # Grayscale SAR image\n        s2_image = Image.open(self.s2_paths[idx]).convert('RGB')  # Color Optical image\n        label = self.labels[idx]  # Numeric label\n\n        if self.transform:\n            s1_image = self.transform(s1_image)\n            s2_image = color_transform(s2_image)\n\n        return s1_image, s2_image, label  # Return label with images\n\n# Define data transformations with normalization\ntransform = transforms.Compose([\n    transforms.Resize((256, 256)),  # Resize to 256x256\n    transforms.ToTensor(),\n    transforms.Normalize(mean=[0.5], std=[0.5])  # Normalize grayscale (SAR) images\n])\n\ncolor_transform = transforms.Compose([\n    transforms.Resize((256, 256)),\n    transforms.ToTensor(),\n    transforms.Normalize(mean=[0.5, 0.5, 0.5], std=[0.5, 0.5, 0.5])  # Normalize color (Optical) images\n])\n\n# Load the dataset and define the DataLoader\ntrain_dataset = SARImageDataset(base_path, transform=transform)\ntrain_loader = DataLoader(train_dataset, batch_size=8, shuffle=True)  # Reduced batch size to 8 for memory efficiency\n\n# Define DCT Residual Block\nclass DCTResidualBlock(nn.Module):\n    def __init__(self, in_channels, out_channels):\n        super(DCTResidualBlock, self).__init__()\n        self.conv1 = nn.Conv2d(in_channels, out_channels, kernel_size=3, padding=1)\n        self.bn1 = nn.BatchNorm2d(out_channels)\n        self.relu = nn.ReLU(inplace=True)\n        self.conv2 = nn.Conv2d(out_channels, out_channels, kernel_size=3, padding=1)\n        self.bn2 = nn.BatchNorm2d(out_channels)\n        self.dct = nn.Conv2d(in_channels, out_channels, kernel_size=1)  # Placeholder for DCT\n        \n    def forward(self, x):\n        dct_features = self.dct(x)\n        x = self.relu(self.bn1(self.conv1(x)))\n        x = self.bn2(self.conv2(x))\n        return self.relu(x + dct_features)\n\n# Define Light-ASPP\nclass LightASPP(nn.Module):\n    def __init__(self, in_channels, out_channels):\n        super(LightASPP, self).__init__()\n        self.conv1 = nn.Conv2d(in_channels, out_channels, kernel_size=1)\n        self.atrous_block1 = nn.Conv2d(in_channels, out_channels, kernel_size=3, padding=1, dilation=1)\n        self.atrous_block2 = nn.Conv2d(in_channels, out_channels, kernel_size=3, padding=6, dilation=6)\n        self.atrous_block3 = nn.Conv2d(in_channels, out_channels, kernel_size=3, padding=12, dilation=12)\n        self.global_avg_pool = nn.AdaptiveAvgPool2d((1, 1))\n        self.conv2 = nn.Conv2d(in_channels, out_channels, kernel_size=1)\n\n    def forward(self, x):\n        x1 = self.conv1(x)\n        x2 = self.atrous_block1(x)\n        x3 = self.atrous_block2(x)\n        x4 = self.atrous_block3(x)\n        x5 = self.global_avg_pool(x)\n        x5 = self.conv2(x5)\n        return x1 + x2 + x3 + x4 + x5\n\n# Define CCMB\nclass CCMB(nn.Module):\n    def __init__(self, in_channels):\n        super(CCMB, self).__init__()\n        self.global_avg_pool = nn.AdaptiveAvgPool2d((1, 1))\n        self.fc = nn.Linear(in_channels, 3)  # Mapping to 3 color channels (RGB)\n\n    def forward(self, x):\n        x = self.global_avg_pool(x)\n        x = torch.flatten(x, 1)\n        color_info = self.fc(x)\n        return color_info.view(-1, 3, 1, 1)\n\n# Define Generator with skip connections (U-Net style)\nclass Generator(nn.Module):\n    def __init__(self):\n        super(Generator, self).__init__()\n        # Encoder\n        self.encoder1 = DCTResidualBlock(1, 32)\n        self.encoder2 = DCTResidualBlock(32, 64)\n        self.encoder3 = LightASPP(64, 128)\n        self.encoder4 = DCTResidualBlock(128, 256)\n\n        # Decoder with skip connections\n        self.decoder1 = nn.ConvTranspose2d(256, 128, kernel_size=4, stride=2, padding=1)\n        self.decoder2 = nn.ConvTranspose2d(128, 64, kernel_size=4, stride=2, padding=1)\n        self.decoder3 = nn.ConvTranspose2d(64, 32, kernel_size=4, stride=2, padding=1)\n        self.final_layer = nn.Conv2d(32, 3, kernel_size=3, stride=1, padding=1)  # Final layer to output RGB\n\n    def forward(self, x):\n        # Encode\n        e1 = self.encoder1(x)\n        e2 = self.encoder2(e1)\n        e3 = self.encoder3(e2)\n        encoded = self.encoder4(e3)\n        \n        # Decoder\n        d1 = F.relu(self.decoder1(encoded))\n        d2 = F.relu(self.decoder2(d1))\n        d3 = F.relu(self.decoder3(d2))\n        decoded = self.final_layer(d3)\n\n        # Resize to 256x256 if needed\n        decoded = F.interpolate(decoded, size=(256, 256), mode='bilinear', align_corners=True)\n\n        return decoded\n\n# Define Discriminator\nclass Discriminator(nn.Module):\n    def __init__(self):\n        super(Discriminator, self).__init__()\n        self.model = nn.Sequential(\n            nn.Conv2d(3, 64, kernel_size=4, stride=2, padding=1),\n            nn.LeakyReLU(0.2),\n            nn.Conv2d(64, 128, kernel_size=4, stride=2, padding=1),\n            nn.BatchNorm2d(128),\n            nn.LeakyReLU(0.2),\n            nn.Conv2d(128, 256, kernel_size=4, stride=2, padding=1),\n            nn.BatchNorm2d(256),\n            nn.LeakyReLU(0.2),\n            nn.Conv2d(256, 512, kernel_size=4, stride=1, padding=1),\n            nn.BatchNorm2d(512),\n            nn.LeakyReLU(0.2),\n            nn.Conv2d(512, 1, kernel_size=4, stride=1, padding=1)\n        )\n\n    def forward(self, x):\n        return self.model(x)\n\n# Simple Classifier for image categorization\nclass Classifier(nn.Module):\n    def __init__(self, num_classes=4):\n        super(Classifier, self).__init__()\n        \n        # First Convolutional block\n        self.conv1 = nn.Conv2d(3, 32, kernel_size=3, stride=1, padding=1)\n        self.bn1 = nn.BatchNorm2d(32)\n        self.pool1 = nn.MaxPool2d(2, 2)  # Reduces size to 128x128\n        \n        # Second Convolutional block\n        self.conv2 = nn.Conv2d(32, 64, kernel_size=3, stride=1, padding=1)\n        self.bn2 = nn.BatchNorm2d(64)\n        self.pool2 = nn.MaxPool2d(2, 2)  # Reduces size to 64x64\n        \n        # Third Convolutional block\n        self.conv3 = nn.Conv2d(64, 128, kernel_size=3, stride=1, padding=1)\n        self.bn3 = nn.BatchNorm2d(128)\n        self.pool3 = nn.MaxPool2d(2, 2)  # Reduces size to 32x32\n        \n        # Fully Connected Layers\n        self.fc1 = nn.Linear(128 * 32 * 32, 256)\n        self.fc2 = nn.Linear(256, num_classes)\n        \n        # Dropout to prevent overfitting\n        self.dropout = nn.Dropout(0.4)\n\n    def forward(self, x):\n        # Apply convolutional blocks\n        x = F.relu(self.bn1(self.conv1(x)))\n        x = self.pool1(x)\n        \n        x = F.relu(self.bn2(self.conv2(x)))\n        x = self.pool2(x)\n        \n        x = F.relu(self.bn3(self.conv3(x)))\n        x = self.pool3(x)\n        \n        # Flatten for fully connected layers\n        x = x.view(x.size(0), -1)  # Flatten the tensor\n        \n        # Fully connected layers with dropout\n        x = F.relu(self.fc1(x))\n        x = self.dropout(x)\n        x = self.fc2(x)\n        \n        return x\n\n# Perceptual Loss (VGG)\nclass VGGPerceptualLoss(nn.Module):\n    def __init__(self):\n        super(VGGPerceptualLoss, self).__init__()\n        vgg = models.vgg19(weights=models.VGG19_Weights.IMAGENET1K_V1).features[:16].eval().cuda()\n        for param in vgg.parameters():\n            param.requires_grad = False\n        self.vgg = vgg\n\n    def forward(self, x, y):\n        x_vgg = self.vgg(x)\n        y_vgg = self.vgg(y)\n        return F.mse_loss(x_vgg, y_vgg)\n        \n# Define the unnormalize function\ndef unnormalize(tensor, mean, std):\n    \"\"\"Unnormalize a tensor image to [0, 1].\"\"\"\n    for t, m, s in zip(tensor, mean, std):\n        t.mul_(s).add_(m)  # Inverse of normalization\n    return tensor.clamp(0, 1)\n\n# Loss functions\ncriterion_GAN = nn.BCEWithLogitsLoss()\ncriterion_pixelwise = nn.L1Loss()\ncriterion_classifier = nn.CrossEntropyLoss()  # For classification task\n\n# Instantiate models\ngenerator = Generator().cuda()\ndiscriminator = Discriminator().cuda()\nclassifier = Classifier().cuda()\nvgg_loss = VGGPerceptualLoss().cuda()\n\n# Optimizers for generator, discriminator, and classifier\noptimizer_G = optim.Adam(generator.parameters(), lr=0.0002, betas=(0.5, 0.999))\noptimizer_D = optim.Adam(discriminator.parameters(), lr=0.0002, betas=(0.5, 0.999))\noptimizer_C = optim.Adam(classifier.parameters(), lr=0.0002)\n\n# Mixed precision training with GradScaler\nscaler = GradScaler()\n\n# Load pre-trained generator and discriminator if resuming\nstart_epoch = 36\ncheckpoint_path = os.path.join(r'/kaggle/input/dct/pytorch/default/1', f'model_epoch_{start_epoch}.pth')\nif os.path.exists(checkpoint_path):\n    checkpoint = torch.load(checkpoint_path)\n    generator.load_state_dict(checkpoint['generator'])\n    discriminator.load_state_dict(checkpoint['discriminator'])\n    classifier.load_state_dict(checkpoint['classifier'])\n    print(\"Loaded pretrained models from previous epoch checkpoint :)\")\n\n# Training loop\nepochs = 100\nsample_interval = 500\n\nfor epoch in range(epochs):\n    for i, (s1, s2, label) in enumerate(train_loader):\n        s1, s2, label = s1.cuda(), s2.cuda(), label.cuda()\n\n        # ------------------ Train Generator ------------------\n        optimizer_G.zero_grad()\n        with torch.amp.autocast(device_type='cuda'):\n            fake_s2 = generator(s1)\n            pred_fake = discriminator(fake_s2)\n            gan_loss = criterion_GAN(pred_fake, torch.ones_like(pred_fake))\n            pixel_loss = criterion_pixelwise(fake_s2, s2)\n            perceptual_loss = vgg_loss(fake_s2, s2)\n            g_loss = gan_loss + pixel_loss + perceptual_loss\n            \n        scaler.scale(g_loss).backward()\n        scaler.step(optimizer_G)\n        scaler.update()\n\n        # ------------------- Train Discriminator -------------------\n        optimizer_D.zero_grad()\n        with torch.amp.autocast(device_type='cuda'):\n            pred_real = discriminator(s2)\n            pred_fake = discriminator(fake_s2.detach())\n            real_loss = criterion_GAN(pred_real, torch.ones_like(pred_real))\n            fake_loss = criterion_GAN(pred_fake, torch.zeros_like(pred_fake))\n            d_loss = (real_loss + fake_loss) / 2\n        scaler.scale(d_loss).backward()\n        \n        scaler.step(optimizer_D)\n        scaler.update()\n\n        # ------------------- Train Classifier -------------------\n        optimizer_C.zero_grad()\n        with torch.amp.autocast(device_type='cuda'):  # Specify device_type:\n            pred_class = classifier(fake_s2.detach())  # Detach here\n            class_loss = criterion_classifier(pred_class, label)\n\n        scaler.scale(class_loss).backward()\n        scaler.step(optimizer_C)\n        scaler.update()\n\n        # Log progress and visualize images\n        if i % sample_interval == 0:\n            print(f\"[Epoch {start_epoch}/{epochs}] [Batch {i}/{len(train_loader)}] [G loss: {g_loss.item():.4f}] [D loss: {d_loss.item():.4f}] [Class loss: {class_loss.item():.4f}]\")\n\n            # Show sample images with predicted and actual classes as titles\n            fig, axs = plt.subplots(1, 3, figsize=(12, 4))\n            axs[0].imshow(s1[0].cpu().permute(1, 2, 0), cmap='gray')\n            axs[0].set_title('Grayscale SAR Image')\n            generated_image_unnormalized = unnormalize(fake_s2[0].cpu().detach(), \n                                                        mean=[0.5, 0.5, 0.5], \n                                                        std=[0.5, 0.5, 0.5])\n            axs[1].imshow(generated_image_unnormalized.permute(1, 2, 0))\n            pred_label = torch.argmax(pred_class[0]).item()\n            axs[1].set_title(f'Predicted: {pred_label}, Actual: {label[0].item()}')\n            s2_unnormalized = unnormalize(s2[0].cpu().detach(), \n                                          mean=[0.5, 0.5, 0.5], \n                                          std=[0.5, 0.5, 0.5])\n            axs[2].imshow(s2_unnormalized.permute(1, 2, 0))\n            axs[2].set_title('Ground Truth')\n            plt.show()\n\n    # Save model checkpoints\n    checkpoint_path = os.path.join(checkpoint_dir, f'model_epoch_{start_epoch}.pth')\n    torch.save({\n        'generator': generator.state_dict(),\n        'discriminator': discriminator.state_dict(),\n        'classifier': classifier.state_dict(),\n    }, checkpoint_path)\n    print(f\"Checkpoint saved for epoch {start_epoch}.\")\n    start_epoch+=1\n    \nprint('Training Completed, Sir!')","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","execution":{"iopub.status.busy":"2024-10-13T20:40:59.201718Z","iopub.execute_input":"2024-10-13T20:40:59.202072Z"},"jupyter":{"source_hidden":true},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Use This, if you only wanna train Generator & Discriminator.","metadata":{}},{"cell_type":"code","source":"# Necessary imports\nimport os\nimport torch\nfrom torch import nn\nimport torch.optim as optim\nfrom torch.utils.data import DataLoader, Dataset\nfrom torchvision import transforms, models\nimport torch.nn.functional as F\nfrom PIL import Image\nimport matplotlib.pyplot as plt\nfrom torch.amp import GradScaler, autocast\n\n# Kaggle paths for dataset and checkpoint\nbase_path = r\"/kaggle/input/sentinel12-image-pairs-segregated-by-terrain/v_2\"\ncheckpoint_dir = r\"/kaggle/working/SAR_Model\"\nif not os.path.exists(checkpoint_dir):\n    os.makedirs(checkpoint_dir)\n\n# Dataset class to handle both SAR (grayscale) and optical (RGB) image pairs\nclass SARImageDataset(Dataset):\n    def __init__(self, base_path, transform=None):\n        self.s1_paths = []\n        self.s2_paths = []\n        self.labels = []\n\n        categories = ['agri', 'barrenland', 'grassland', 'urban']\n        category_to_label = {cat: i for i, cat in enumerate(categories)}\n        \n        for category in categories:\n            s1_folder = os.path.join(base_path, category, 's1')\n            s2_folder = os.path.join(base_path, category, 's2')\n            self.s1_paths += sorted([os.path.join(s1_folder, img) for img in os.listdir(s1_folder)])\n            self.s2_paths += sorted([os.path.join(s2_folder, img) for img in os.listdir(s2_folder)])\n            self.labels += [category_to_label[category]] * len(os.listdir(s1_folder))\n        \n        self.transform = transform\n\n    def __len__(self):\n        return len(self.s1_paths)\n\n    def __getitem__(self, idx):\n        s1_image = Image.open(self.s1_paths[idx]).convert('L')  # Grayscale SAR image\n        s2_image = Image.open(self.s2_paths[idx]).convert('RGB')  # Color Optical image\n        label = self.labels[idx]  # Numeric label\n\n        if self.transform:\n            s1_image = self.transform(s1_image)\n            s2_image = color_transform(s2_image)\n\n        return s1_image, s2_image, label  # Return label with images\n\n# Define data transformations with normalization\ntransform = transforms.Compose([\n    transforms.Resize((256, 256)),  # Resize to 256x256\n    transforms.ToTensor(),\n    transforms.Normalize(mean=[0.5], std=[0.5])  # Normalize grayscale (SAR) images\n])\n\ncolor_transform = transforms.Compose([\n    transforms.Resize((256, 256)),\n    transforms.ToTensor(),\n    transforms.Normalize(mean=[0.5, 0.5, 0.5], std=[0.5, 0.5, 0.5])  # Normalize color (Optical) images\n])\n\n# Load the dataset and define the DataLoader\ntrain_dataset = SARImageDataset(base_path, transform=transform)\ntrain_loader = DataLoader(train_dataset, batch_size=8, shuffle=True)  # Reduced batch size to 8 for memory efficiency\n\n# Define DCT Residual Block\nclass DCTResidualBlock(nn.Module):\n    def __init__(self, in_channels, out_channels):\n        super(DCTResidualBlock, self).__init__()\n        self.conv1 = nn.Conv2d(in_channels, out_channels, kernel_size=3, padding=1)\n        self.bn1 = nn.BatchNorm2d(out_channels)\n        self.relu = nn.ReLU(inplace=True)\n        self.conv2 = nn.Conv2d(out_channels, out_channels, kernel_size=3, padding=1)\n        self.bn2 = nn.BatchNorm2d(out_channels)\n        self.dct = nn.Conv2d(in_channels, out_channels, kernel_size=1)  # Placeholder for DCT\n        \n    def forward(self, x):\n        dct_features = self.dct(x)\n        x = self.relu(self.bn1(self.conv1(x)))\n        x = self.bn2(self.conv2(x))\n        return self.relu(x + dct_features)\n\n# Define Light-ASPP\nclass LightASPP(nn.Module):\n    def __init__(self, in_channels, out_channels):\n        super(LightASPP, self).__init__()\n        self.conv1 = nn.Conv2d(in_channels, out_channels, kernel_size=1)\n        self.atrous_block1 = nn.Conv2d(in_channels, out_channels, kernel_size=3, padding=1, dilation=1)\n        self.atrous_block2 = nn.Conv2d(in_channels, out_channels, kernel_size=3, padding=6, dilation=6)\n        self.atrous_block3 = nn.Conv2d(in_channels, out_channels, kernel_size=3, padding=12, dilation=12)\n        self.global_avg_pool = nn.AdaptiveAvgPool2d((1, 1))\n        self.conv2 = nn.Conv2d(in_channels, out_channels, kernel_size=1)\n\n    def forward(self, x):\n        x1 = self.conv1(x)\n        x2 = self.atrous_block1(x)\n        x3 = self.atrous_block2(x)\n        x4 = self.atrous_block3(x)\n        x5 = self.global_avg_pool(x)\n        x5 = self.conv2(x5)\n        return x1 + x2 + x3 + x4 + x5\n\n# Define CCMB\nclass CCMB(nn.Module):\n    def __init__(self, in_channels):\n        super(CCMB, self).__init__()\n        self.global_avg_pool = nn.AdaptiveAvgPool2d((1, 1))\n        self.fc = nn.Linear(in_channels, 3)  # Mapping to 3 color channels (RGB)\n\n    def forward(self, x):\n        x = self.global_avg_pool(x)\n        x = torch.flatten(x, 1)\n        color_info = self.fc(x)\n        return color_info.view(-1, 3, 1, 1)\n\n# Define Generator with skip connections (U-Net style)\nclass Generator(nn.Module):\n    def __init__(self):\n        super(Generator, self).__init__()\n        # Encoder\n        self.encoder1 = DCTResidualBlock(1, 32)\n        self.encoder2 = DCTResidualBlock(32, 64)\n        self.encoder3 = LightASPP(64, 128)\n        self.encoder4 = DCTResidualBlock(128, 256)\n\n        # Decoder with skip connections\n        self.decoder1 = nn.ConvTranspose2d(256, 128, kernel_size=4, stride=2, padding=1)\n        self.decoder2 = nn.ConvTranspose2d(128, 64, kernel_size=4, stride=2, padding=1)\n        self.decoder3 = nn.ConvTranspose2d(64, 32, kernel_size=4, stride=2, padding=1)\n        self.final_layer = nn.Conv2d(32, 3, kernel_size=3, stride=1, padding=1)  # Final layer to output RGB\n\n    def forward(self, x):\n        # Encode\n        e1 = self.encoder1(x)\n        e2 = self.encoder2(e1)\n        e3 = self.encoder3(e2)\n        encoded = self.encoder4(e3)\n        \n        # Decoder\n        d1 = F.relu(self.decoder1(encoded))\n        d2 = F.relu(self.decoder2(d1))\n        d3 = F.relu(self.decoder3(d2))\n        decoded = self.final_layer(d3)\n\n        # Resize to 256x256 if needed\n        decoded = F.interpolate(decoded, size=(256, 256), mode='bilinear', align_corners=True)\n\n        return decoded\n\n# Define Discriminator\nclass Discriminator(nn.Module):\n    def __init__(self):\n        super(Discriminator, self).__init__()\n        self.model = nn.Sequential(\n            nn.Conv2d(3, 64, kernel_size=4, stride=2, padding=1),\n            nn.LeakyReLU(0.2),\n            nn.Conv2d(64, 128, kernel_size=4, stride=2, padding=1),\n            nn.BatchNorm2d(128),\n            nn.LeakyReLU(0.2),\n            nn.Conv2d(128, 256, kernel_size=4, stride=2, padding=1),\n            nn.BatchNorm2d(256),\n            nn.LeakyReLU(0.2),\n            nn.Conv2d(256, 512, kernel_size=4, stride=1, padding=1),\n            nn.BatchNorm2d(512),\n            nn.LeakyReLU(0.2),\n            nn.Conv2d(512, 1, kernel_size=4, stride=1, padding=1)\n        )\n\n    def forward(self, x):\n        return self.model(x)\n\n# Simple Classifier for image categorization\nclass Classifier(nn.Module):\n    def __init__(self, num_classes=4):\n        super(Classifier, self).__init__()\n        \n        # First Convolutional block\n        self.conv1 = nn.Conv2d(3, 32, kernel_size=3, stride=1, padding=1)\n        self.bn1 = nn.BatchNorm2d(32)\n        self.pool1 = nn.MaxPool2d(2, 2)  # Reduces size to 128x128\n        \n        # Second Convolutional block\n        self.conv2 = nn.Conv2d(32, 64, kernel_size=3, stride=1, padding=1)\n        self.bn2 = nn.BatchNorm2d(64)\n        self.pool2 = nn.MaxPool2d(2, 2)  # Reduces size to 64x64\n        \n        # Third Convolutional block\n        self.conv3 = nn.Conv2d(64, 128, kernel_size=3, stride=1, padding=1)\n        self.bn3 = nn.BatchNorm2d(128)\n        self.pool3 = nn.MaxPool2d(2, 2)  # Reduces size to 32x32\n        \n        # Fully Connected Layers\n        self.fc1 = nn.Linear(128 * 32 * 32, 256)\n        self.fc2 = nn.Linear(256, num_classes)\n        \n        # Dropout to prevent overfitting\n        self.dropout = nn.Dropout(0.4)\n\n    def forward(self, x):\n        # Apply convolutional blocks\n        x = F.relu(self.bn1(self.conv1(x)))\n        x = self.pool1(x)\n        \n        x = F.relu(self.bn2(self.conv2(x)))\n        x = self.pool2(x)\n        \n        x = F.relu(self.bn3(self.conv3(x)))\n        x = self.pool3(x)\n        \n        # Flatten for fully connected layers\n        x = x.view(x.size(0), -1)  # Flatten the tensor\n        \n        # Fully connected layers with dropout\n        x = F.relu(self.fc1(x))\n        x = self.dropout(x)\n        x = self.fc2(x)\n        \n        return x\n\n# Perceptual Loss (VGG)\nclass VGGPerceptualLoss(nn.Module):\n    def __init__(self):\n        super(VGGPerceptualLoss, self).__init__()\n        vgg = models.vgg19(weights=models.VGG19_Weights.IMAGENET1K_V1).features[:16].eval().cuda()\n        for param in vgg.parameters():\n            param.requires_grad = False\n        self.vgg = vgg\n\n    def forward(self, x, y):\n        x_vgg = self.vgg(x)\n        y_vgg = self.vgg(y)\n        return F.mse_loss(x_vgg, y_vgg)\n        \n# Define the unnormalize function\ndef unnormalize(tensor, mean, std):\n    \"\"\"Unnormalize a tensor image to [0, 1].\"\"\"\n    for t, m, s in zip(tensor, mean, std):\n        t.mul_(s).add_(m)\n    return tensor.clamp(0, 1) \n\n# Loss functions\ncriterion_GAN = nn.BCEWithLogitsLoss()\ncriterion_pixelwise = nn.L1Loss()\n\n# Instantiate models\ngenerator = Generator().cuda()\ndiscriminator = Discriminator().cuda()\nclassifier = Classifier().cuda()\nvgg_loss = VGGPerceptualLoss().cuda()\n\n# Optimizers for generator, discriminator, and classifier\noptimizer_G = optim.Adam(generator.parameters(), lr=0.0002, betas=(0.5, 0.999))\noptimizer_D = optim.Adam(discriminator.parameters(), lr=0.0002, betas=(0.5, 0.999))\n\n# Mixed precision training with GradScaler\nscaler = GradScaler()\n\n# Load pre-trained generator and discriminator if resuming\nstart_epoch = 36\ncheckpoint_path = os.path.join(r'/kaggle/input/dct/pytorch/default/1', f'model_epoch_{start_epoch-1}.pth')\nif os.path.exists(checkpoint_path):\n    checkpoint = torch.load(checkpoint_path,weights_only=True)\n    generator.load_state_dict(checkpoint['generator'])\n    discriminator.load_state_dict(checkpoint['discriminator'])\n    classifier.load_state_dict(checkpoint['classifier'])\n    print(f\"Loaded pretrained models from previous epoch {start_epoch-1} checkpoint :)\")\n\n# Training loop\nepochs = 100\nsample_interval = 10\n\nfor epoch in range(epochs):\n    for i, (s1, s2, label) in enumerate(train_loader):\n        s1, s2, label = s1.cuda(), s2.cuda(), label.cuda()\n\n        # ------------------ Train Generator ------------------\n        optimizer_G.zero_grad()\n        with autocast('cuda'):\n            fake_s2 = generator(s1)\n            pred_fake = discriminator(fake_s2)\n            gan_loss = criterion_GAN(pred_fake, torch.ones_like(pred_fake))\n            pixel_loss = criterion_pixelwise(fake_s2, s2)\n            perceptual_loss = vgg_loss(fake_s2, s2)\n            g_loss = gan_loss + 100 * pixel_loss + 0.1 * perceptual_loss\n            \n        scaler.scale(g_loss).backward()\n        scaler.step(optimizer_G)\n        scaler.update()\n\n        # ------------------- Train Discriminator -------------------\n        optimizer_D.zero_grad()\n        with autocast('cuda'):\n            pred_real = discriminator(s2)\n            pred_fake = discriminator(fake_s2.detach())\n            real_loss = criterion_GAN(pred_real, torch.ones_like(pred_real))\n            fake_loss = criterion_GAN(pred_fake, torch.zeros_like(pred_fake))\n            d_loss = (real_loss + fake_loss) / 2\n            \n        scaler.scale(d_loss).backward()\n        scaler.step(optimizer_D)\n        scaler.update()\n\n        pred_class = classifier(fake_s2.detach())\n        \n        # Log progress and visualize images\n        if i % sample_interval == 0:\n            print(f\"[Epoch {start_epoch}/{epochs}] [Batch {i}/{len(train_loader)}] [G loss: {g_loss.item():.4f}] [D loss: {d_loss.item():.4f}]\")\n\n            # Show sample images with predicted and actual classes as titles\n            fig, axs = plt.subplots(1, 3, figsize=(12, 4))\n            axs[0].imshow(s1[0].cpu().permute(1, 2, 0), cmap='gray')\n            axs[0].set_title('Grayscale SAR Image')\n            generated_image_unnormalized = unnormalize(fake_s2[0].cpu().detach(), \n                                                        mean=[0.5, 0.5, 0.5], \n                                                        std=[0.5, 0.5, 0.5])\n            axs[1].imshow(generated_image_unnormalized.permute(1, 2, 0))\n            pred_label = torch.argmax(pred_class[0]).item()\n            axs[1].set_title(f'Predicted: {pred_label}, Actual: {label[0].item()}')\n            s2_unnormalized = unnormalize(s2[0].cpu().detach(), \n                                          mean=[0.5, 0.5, 0.5], \n                                          std=[0.5, 0.5, 0.5])\n            axs[2].imshow(s2_unnormalized.permute(1, 2, 0))\n            axs[2].set_title('Ground Truth')\n            plt.show()\n\n    # Save model checkpoints\n    checkpoint_path = os.path.join(checkpoint_dir, f'model_epoch_{start_epoch}.pth')\n    torch.save({\n        'generator': generator.state_dict(),\n        'discriminator': discriminator.state_dict(),\n        'classifier': classifier.state_dict(),\n    }, checkpoint_path)\n    print(f\"Checkpoint saved for epoch {start_epoch}.\")\n    start_epoch+=1\n    \nprint('Training Completed, Sir!')","metadata":{"trusted":true},"execution_count":null,"outputs":[]}]}